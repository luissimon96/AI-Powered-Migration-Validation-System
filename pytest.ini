[tool:pytest]
# ═══════════════════════════════════════════════════════════════
# Enhanced Pytest Configuration for Production-Grade Testing
# ═══════════════════════════════════════════════════════════════

# Test discovery
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Advanced output and reporting
addopts =
    --verbose
    --tb=short
    --strict-markers
    --strict-config
    --cov=src
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-report=xml:coverage.xml
    --cov-fail-under=90
    --cov-branch
    --durations=10
    --durations-min=1.0
    --maxfail=5
    --disable-warnings

# Enhanced test markers
markers =
    # Core test categories
    unit: Unit tests for individual components
    integration: Integration tests for component interactions
    behavioral: Behavioral validation tests using CrewAI and browser automation
    system: System-level end-to-end tests

    # Performance testing
    performance: Performance and load tests
    benchmark: Benchmark tests for performance regression detection
    memory: Memory usage and leak detection tests

    # Security testing
    security: Security and vulnerability tests
    penetration: Penetration testing scenarios
    fuzz: Fuzzing tests for input validation

    # Advanced testing patterns
    property: Property-based tests using Hypothesis
    mutation: Mutation testing validation
    contract: Contract tests for API compatibility
    chaos: Chaos engineering and fault injection tests
    visual: Visual regression tests

    # Execution characteristics
    slow: Slow running tests (>10 seconds)
    external: Tests requiring external services
    llm: Tests requiring LLM API access
    browser: Tests requiring browser automation
    database: Tests requiring database connections
    network: Tests requiring network access

    # Test stability and reliability
    flaky: Tests known to be flaky (temporary marking)
    smoke: Smoke tests for quick validation
    regression: Regression tests for specific bug fixes
    critical: Critical path tests that must always pass

    # Environment specific
    local: Tests that only run in local development
    ci: Tests optimized for CI/CD execution
    production: Tests safe to run against production-like environments

# Async testing configuration
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function

# Test filtering and warnings
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore:.*unclosed.*:ResourceWarning
    ignore::pytest.PytestUnraisableExceptionWarning
    ignore::pytest.PytestCollectionWarning
    error::UserWarning

# Minimum version requirements
minversion = 7.0

# Test collection configuration
collect_ignore = [
    "setup.py",
    "docs/*",
    "build/*",
    "dist/*",
    venv/*
    env/*
    .git/*
    __pycache__/*

# Disable cacheprovider for CI consistency
cache_dir = .pytest_cache

# Test timeout configuration
timeout = 300
timeout_method = thread

# Parallel execution settings
addopts_parallel = -n auto --dist worksteal

# Property-based testing configuration (disabled due to conflicts)
# hypothesis_profile = default
# hypothesis_verbosity = normal
# hypothesis_max_examples = 100
# hypothesis_deadline = 10000

# ═══════════════════════════════════════════════════════════════
# Coverage Configuration
# ═══════════════════════════════════════════════════════════════

[coverage:run]
source = src
omit =
    */tests/*
    */test_*
    */__pycache__/*
    */venv/*
    */env/*
    setup.py
    */migrations/*
    */scripts/*

branch = true
concurrency = thread,multiprocessing

# Coverage data file
data_file = .coverage

[coverage:report]
# Fail if coverage is below threshold
fail_under = 90

# Precision for coverage reporting
precision = 2

# Skip empty files
skip_empty = true

# Show missing lines
show_missing = true

# Sort by coverage percentage
sort = Cover

exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod
    @overload
    # Type checking imports
    if TYPE_CHECKING:
    # Defensive assertions
    assert False
    # Platform specific code
    if sys.platform

[coverage:html]
directory = htmlcov
title = AI Migration Validator Coverage Report
show_contexts = true

[coverage:xml]
output = coverage.xml

# ═══════════════════════════════════════════════════════════════
# Hypothesis Configuration (Property-Based Testing) - DISABLED
# ═══════════════════════════════════════════════════════════════

# [tool:hypothesis]
# max_examples = 100
# deadline = 10000
# verbosity = normal
# database = .hypothesis/examples
# phases = explicit,reuse,generate,target,shrink,explain

# ═══════════════════════════════════════════════════════════════
# Test Execution Profiles
# ═══════════════════════════════════════════════════════════════

# Fast profile for local development
[profile:fast]
addopts = -m "not slow and not external and not llm" --maxfail=3 --tb=line

# Comprehensive profile for CI/CD
[profile:comprehensive]
addopts = --cov --cov-report=html --cov-report=xml --cov-fail-under=90 --durations=20

# Performance profile for performance testing
[profile:performance]
addopts = -m "performance or benchmark" --benchmark-only --durations=0

# Security profile for security testing
[profile:security]
addopts = -m "security or penetration or fuzz" --tb=long

# Smoke profile for quick validation
[profile:smoke]
addopts = -m "smoke or critical" --maxfail=1 --tb=line --disable-warnings